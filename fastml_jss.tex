\documentclass[article]{jss}

%% -- Preamble and metadata (do not modify) -----------------------------------
\author{Selcuk Korkmaz\Thanks{Corresponding author. Email: \email{selcukorkmaz@gmail.com}} \\
        Dincer Goksuluk\\
        Eda Karaismailoglu}
\Plainauthor{Selcuk Korkmaz, Dincer Goksuluk, Eda Karaismailoglu}

\title{fastml: Fast Machine Learning Model Training and Evaluation}
\Plaintitle{fastml: Fast Machine Learning Model Training and Evaluation}
\Shorttitle{fastml}

\Abstract{
\pkg{fastml} streamlines the development of machine learning models in R by combining leakage-guarded resampling, fold-isolated preprocessing, and unified explainability tools built on \pkg{DALEX}. The package offers standardized result objects, multi-model workflows, and hyperparameter tuning with parallel execution, enabling reproducible pipelines that balance speed with statistical rigor.
}

\Keywords{machine learning, resampling, cross-validation, explainable AI, R}
\Plainkeywords{machine learning, resampling, cross-validation, explainable AI, R}

\Address{
  Selcuk Korkmaz\thanks{ORCID: 0000-0003-4632-6850}\\
  Department of Biostatistics\\
  Email: \email{selcukorkmaz@gmail.com}

  \vspace{0.4cm}
  Dincer Goksuluk\thanks{ORCID: 0000-0002-2752-7668}\\
  Department of Biostatistics\\
  Email: \email{dincer.goksuluk@gmail.com}

  \vspace{0.4cm}
  Eda Karaismailoglu\thanks{ORCID: 0000-0003-3085-7809}\\
  Department of Biostatistics\\
  Email: \email{eda.karaismailoglu@sbu.edu.tr}
}

%% -- Article body ------------------------------------------------------------
\begin{document}

\section{Introduction}
The rapid expansion of model-building toolkits in R has improved analyst productivity but has also fragmented workflows across disparate interfaces. Cross-validation, preprocessing, and explainability are often scattered across packages, leaving analysts to hand-stitch pipelines while guarding against data leakage. \pkg{fastml} addresses these challenges by providing a unified grammar for end-to-end machine learning that emphasizes fold-isolated preprocessing, standardized result objects, and explainability via \pkg{DALEX}. The package is built on the \pkg{tidymodels} ecosystem and \pkg{rsample} resampling infrastructure, but it wraps these components into a cohesive interface that lowers the entry barrier for applied users while retaining methodological rigor.

\section{Design Philosophy and Statistical Framework}
\subsection{Leakage-guarded resampling}
A central goal of \pkg{fastml} is to ensure that estimates of out-of-sample performance remain unbiased. All preprocessing---including imputation, encoding, and scaling---is executed separately within each resampling split. Recipes are estimated on the analysis portion of each split and then applied to the corresponding assessment data. This “fold-isolated” design prevents information leakage and keeps performance estimates aligned with deployment conditions.

\subsection{Resampling strategies}
The package exposes a spectrum of resampling options. Standard $K$-fold cross-validation is available alongside repeated CV, validation splits, and nested CV for hyperparameter tuning. Grouped or blocked resampling can be configured using \pkg{rsample} specifications to respect subject-level grouping or temporal structure, while stratification preserves outcome balance for classification tasks. Each strategy yields consistent performance objects that can be aggregated or compared across models.

\subsection{Hyperparameter tuning and model selection}
\pkg{fastml} supports grid and Bayesian optimization for tuning through the \code{tuning\_strategy} argument. Search spaces are derived from \pkg{dials} parameter definitions, and tuning is executed inside the chosen resampling scheme. By default, the package selects the best-performing workflow based on the primary metric (e.g., accuracy or concordance index), yet it also returns full tuning histories, enabling users to audit trade-offs and rerank candidates.

\section{Software Architecture and Data Flow}
\subsection{Pipeline orchestration}
The core pipeline proceeds through data validation, recipe construction, model specification, resampling, tuning, and performance summarization. Internally, \pkg{fastml} builds \pkg{workflows} objects that encapsulate both preprocessing and model components, ensuring that the same transformations are replayed during assessment and prediction. Logging captures seeds, resampling splits, and tuning grids, facilitating reproducibility.

\subsection{Standardized result objects}
The package returns structured S3 objects containing model definitions, preprocessing recipes, resampling artifacts, and fitted workflows. Accessor functions such as \code{summary.fastml()} and \code{plot.fastml()} expose harmonized summaries across model families. Predictions for new data reuse the stored recipe to guarantee consistency, while helper functions like \code{get\_best\_workflows()} provide direct access to the top-performing candidates.

\subsection{Parallelization and reproducibility}
Parallel execution is available through \pkg{future} and \pkg{doFuture}. Users can register a plan (e.g., \code{multisession}) prior to calling \code{fastml()}, and seeds are managed per resample to yield reproducible results. Optional logging of session information and package versions supports long-term reproducibility, especially when deploying models in regulated environments.

\section{Methodology and Algorithms}
\subsection{Supported learners}
\pkg{fastml} ships with curated specifications that cover linear models, tree-based ensembles, support vector machines, $k$-nearest neighbors, neural networks, and survival models (e.g., Cox regression, flexible parametric models via \pkg{flexsurv}, and boosted survival models using \pkg{xgboost}). Default hyperparameter ranges are defined for each learner, and users can override them or supply custom \pkg{parsnip} model specifications.

\subsection{Evaluation metrics}
Classification workflows report accuracy, area under the ROC curve, sensitivity, specificity, and F1 scores, while regression workflows summarize root-mean-squared error, mean absolute error, and $R^2$. Survival analysis uses concordance indices, integrated Brier scores, and restricted mean survival time differences. All metrics are calculated within resampling folds to avoid optimistic bias.

\subsection{Model diagnostics}
Diagnostic visualizations include calibration curves, residual plots for regression, and cumulative hazard comparisons for survival models. Interaction strength and variable importance can be probed through \code{fastexplain()} (Section~\ref{sec:fastexplain}) to highlight influential predictors and potential non-linearities.

\section{Core Functions}
\subsection{Exploratory profiling with \code{fast\_explore()}}
\code{fast\_explore()} provides a rapid overview of dataset structure, reporting missingness, outcome balance, and basic summaries of numeric and categorical predictors. The function is intended as a preflight check to guide recipe selection (e.g., the need for imputation or class balancing).

\subsection{Model training via \code{fastml()}}
\code{fastml()} orchestrates the full modeling pipeline. Users supply a data frame and a target label; optional arguments control resampling design, model sets, tuning strategies, and parallel execution. The function returns a \code{fastml} object containing tuned workflows, performance summaries, and metadata on preprocessing steps.

\subsection{Interpretability with \code{fastexplain()}\label{sec:fastexplain}}
\code{fastexplain()} bridges model training and interpretability. Methods include permutation feature importance, partial dependence, accumulated local effects, individual conditional expectation curves, surrogate trees, SHAP-like decompositions via \pkg{iml}, and counterfactual explanations. For survival models, \code{fastexplain()} can report time-dependent effects and risk group comparisons using integrated Brier scores.

\section{Reproducible Workflow Example}
The following example illustrates a binary classification workflow on the classic \code{iris} dataset. To keep the focus on model-building steps, plotting output is summarized rather than displayed in full.

\begin{CodeChunk}
\begin{CodeInput}
R> library(fastml)
R> data(iris)
R> iris <- subset(iris, Species != "setosa")
R> iris$Species <- factor(iris$Species)
\end{CodeInput}
\end{CodeChunk}

\subsection{Exploration}
\begin{CodeChunk}
\begin{CodeInput}
R> fast_explore(iris, label = "Species")
\end{CodeInput}
\begin{CodeOutput}
# Summaries of class balance, missingness, and basic distributions
\end{CodeOutput}
\end{CodeChunk}

\subsection{Model training}
We compare a penalized logistic regression and a random forest with Bayesian tuning under repeated cross-validation.

\begin{CodeChunk}
\begin{CodeInput}
R> set.seed(123)
R> model_fit <- fastml(
+   data = iris,
+   label = "Species",
+   models = c("glmnet", "rand_forest"),
+   resamples = rsample::vfold_cv(iris, v = 5, repeats = 3, strata = "Species"),
+   tuning_strategy = "bayes",
+   tuning_iterations = 10,
+   metrics = yardstick::metric_set(accuracy, roc_auc)
+ )
\end{CodeInput}
\end{CodeChunk}

\subsection{Performance review}
\begin{CodeChunk}
\begin{CodeInput}
R> summary(model_fit)
\end{CodeInput}
\begin{CodeOutput}
# Displays resampled accuracy and ROC AUC for each candidate model,
# highlighting the best-performing workflow.
\end{CodeOutput}
\end{CodeChunk}

\subsection{Explainability}
\begin{CodeChunk}
\begin{CodeInput}
R> fastexplain(model_fit, method = "ale", features = "Petal.Length")
\end{CodeInput}
\begin{CodeOutput}
# Returns accumulated local effect curves summarizing feature impact.
\end{CodeOutput}
\end{CodeChunk}

\subsection{Deployment}
Once a model is selected, predictions on new data are computed with \code{predict()} which automatically applies the stored recipe. Persisted objects can be saved via \code{save.fastml()} to support deployment pipelines.

\section{Explainability and Model Insight}
Explainability modules leverage \pkg{DALEX} for model-agnostic diagnostics. Permutation feature importance ranks predictors by their effect on resampled performance. Partial dependence and accumulated local effects capture global trends while preserving non-linear relationships. Local methods such as LIME and counterfactual explanations expose instance-level reasoning, enabling practitioners to audit fairness or identify brittle regions of the predictor space. For survival models, \pkg{fastml} integrates time-varying risk assessments and visualizations of survival curves across covariate profiles.

\section{Comparison with Related Software}
\subsection{Conceptual differences}
\pkg{caret} provides a broad catalogue of models with a unified interface, but preprocessing steps often require manual coordination to avoid leakage. \pkg{tidymodels} offers modular tools for recipes, resampling, and tuning, yet it leaves most orchestration to the user. \pkg{mlr3} emphasizes a graph-based pipeline that excels at extensibility but can be verbose for routine analyses. \pkg{fastml} prioritizes turnkey workflows with explicit leakage guards and integrated explainability, reducing boilerplate while retaining transparency into underlying \pkg{tidymodels} components.

\subsection{Feature comparison}
\begin{table}[t]
  \centering
  \begin{tabular}{p{3cm}p{2.5cm}p{2.5cm}p{2.5cm}p{2.5cm}}
    \hline
    Feature & \pkg{fastml} & \pkg{caret} & \pkg{tidymodels} & \pkg{mlr3} \\
    \hline
    API style & Turnkey workflows & Unified wrappers & Modular grammar & Graph-based pipeline \\
    Leakage protection & Fold-isolated recipes & User-managed & Recipe-aware & User-managed \\
    Nested CV support & Built-in tuning & Limited & Available via \pkg{tune} & Available via resampling graphs \\
    Explainability & \pkg{DALEX}, ICE, counterfactuals & Limited & Add-ons (e.g., \pkg{DALEX}) & Add-ons (e.g., \pkg{iml}) \\
    Automation & Auto-tuned multi-model runs & Manual & Flexible but verbose & Strong batch tools \\
    Output structure & Standardized S3 object & Model-specific & Tibbles of results & Task/Learner/Resample objects \\
    \hline
  \end{tabular}
  \caption{Comparison of \pkg{fastml} with related machine learning toolkits in R.}
\end{table}

\section{Discussion and Conclusion}
\pkg{fastml} combines leakage-guarded resampling, fold-isolated preprocessing, and unified explainability into a concise interface. By standardizing result objects and leveraging \pkg{tidymodels} components, it reduces boilerplate without obscuring the underlying workflow. Limitations include reliance on \pkg{tidymodels} defaults for some model engines and the current focus on single-outcome supervised learning. Future work will extend support for multitask and probabilistic forecasting, add automated fairness diagnostics, and broaden survival modeling options. The design choices in \pkg{fastml} demonstrate that principled defaults and transparent pipelines can coexist, enabling analysts to prototype rapidly while preserving statistical validity.

\section*{Acknowledgments}
We thank the \pkg{fastml} user community for feedback on early releases and the maintainers of the \pkg{tidymodels} and \pkg{DALEX} ecosystems for foundational tools.

\bibliography{fastml}

\end{document}
