[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fastml",
    "section": "",
    "text": "Welcome to fastml, your one-stop R package for lightning-fast model development, evaluation, and explainabilityâ€”all in a single workflow."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "fastml",
    "section": "ğŸ’¿ Installation",
    "text": "ğŸ’¿ Installation\ninstall.packages(\"fastml\")"
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "fastml",
    "section": "ğŸš€ Get Started",
    "text": "ğŸš€ Get Started\nIf youâ€™re new to fastml, begin with our Getting Started Guide\n\n\n\ny = 10\nx = 5\nmodel &lt;- x + y\nhist(rnorm(100))\n\n\n\n\n\n\n\n\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type â€˜license()â€™ or â€˜licence()â€™ for distribution details.\n\n\nDsadkhjab kjfhbksjd bfhakjdsbhfjk asdbhkjf\n\n\n\n\ny = 10\nx = 5\nmodel &lt;- x + y\nhist(rnorm(100))\n\n\n\n\n\n\n\n\n\nR is free software and comes with ABSOLUTELY NO WARRANTY. You are welcome to redistribute it under certain conditions. Type â€˜license()â€™ or â€˜licence()â€™ for distribution details."
  },
  {
    "objectID": "reference/index.html",
    "href": "reference/index.html",
    "title": "Reference",
    "section": "",
    "text": "How to use this reference\n\n\n\n\n\n\nClick any function name to see its complete help page.\nSections are grouped so you can quickly locate functions for each step of the workflow.\n\nThese pages are generated from roxygen2 docs, so they stay in sync with each release."
  },
  {
    "objectID": "reference/index.html#core-workflow-verbs",
    "href": "reference/index.html#core-workflow-verbs",
    "title": "Reference",
    "section": "Core workflow verbs",
    "text": "Core workflow verbs\n\n\n\nFunction\nPurpose\n\n\n\n\nfastml()\nTrain, tune & compare many models in one call.\n\n\nfastexplore()\nOne-shot EDA: missings, outliers, correlations, plots.\n\n\nfastexplain()\nPermutation VI, SHAP values, profiles & calibration."
  },
  {
    "objectID": "reference/index.html#model-training-helpers",
    "href": "reference/index.html#model-training-helpers",
    "title": "Reference",
    "section": "Model-training helpers",
    "text": "Model-training helpers\n\n\n\n\n\n\n\nFunction\nDescription\n\n\n\n\ntrain_models()\nInternal engine powering fastml (multi-engine, tuning).\n\n\nevaluate_models()\nComputes metrics, collects predictions, highlights best.\n\n\navailableMethods()\nReturns keys for all supported algorithms."
  },
  {
    "objectID": "reference/index.html#specification-builders",
    "href": "reference/index.html#specification-builders",
    "title": "Reference",
    "section": "Specification builders",
    "text": "Specification builders\nAll exported so you can use them directly:\ndefine_rand_forest_spec() define_lightgbm_spec() define_xgboost_spec()\ndefine_decision_tree_spec() define_svm_linear_spec() define_svm_rbf_spec()\nâ€¦and the rest."
  },
  {
    "objectID": "reference/index.html#utility-helpers",
    "href": "reference/index.html#utility-helpers",
    "title": "Reference",
    "section": "Utility helpers",
    "text": "Utility helpers\n\n\n\n\n\n\n\nHelper\nWhat it does\n\n\n\n\nsanitize()\nClean column names / vectors (spaces â†’ _, remove /, etc.).\n\n\nget_default_engine()\nMap algorithm â†’ default engine.\n\n\nget_default_params()\nSensible defaults when you skip tuning.\n\n\nget_default_tune_params()\nLightweight grids when tune = TRUE.\n\n\nget_engine_names()\nExtract engine labels from nested workflows.\n\n\n\n\n\nLooking for step-by-step walkthroughs?\nVisit the Tutorials section for narrative, hands-on guides that combine many of the functions listed above."
  },
  {
    "objectID": "tutorials/fastml-advanced.html",
    "href": "tutorials/fastml-advanced.html",
    "title": "Advanced Workflows with fastml()",
    "section": "",
    "text": "# Why an Advanced Tutorial?\nfastml() handles 90â€¯% of dayâ€‘toâ€‘day modelling outâ€‘ofâ€‘theâ€‘box, but real projects often need more control. This guide dives into the powerâ€‘user levers:\n\nCustom recipes (feature engineering, text / time features)\nAlternative hyperâ€‘parameter search strategies (Bayesian, adaptive racing)\nParallel and distributed engines (multiâ€‘core, sparklyr, h2o)\nStacking / ensembling multiple fastml runs\nLearning curves and automated model monitoring\n\nWe assume youâ€™ve worked through the classification and regression tutorials.\n\n## 1 Â· Setâ€‘up\nlibrary(fastml)\nlibrary(tidymodels)\nlibrary(dplyr)\n\ndata(\"credit_data\", package = \"modeldata\")\ncredit_data &lt;- credit_data %&gt;%\nmutate(Status = factor(Status, levels = c(\"good\", \"bad\")))\nWeâ€™ll predict credit status (binary classification). The dataset contains wide ordinals, categoricals, and numeric predictors â€” perfect for advanced preprocessing.\n\n## 2 Â· Custom Recipe\nBelow we:\n\nImpute missing numerics with KNN; categoricals with the mode.\nCreate interaction terms (Income Ã— Limit).\nNormalize numeric predictors.\nCollapse infrequent factor levels (&lt;â€¯3â€¯%).\n\ncredit_rec &lt;- recipe(Status ~ ., data = credit_data) %&gt;%\n  step_impute_knn(all_numeric_predictors()) %&gt;%\n  step_impute_mode(all_nominal_predictors()) %&gt;%\n  step_interact(terms = ~ Income:Limit) %&gt;%\n  step_other(all_nominal_predictors(), threshold = 0.03) %&gt;%\n  step_dummy(all_nominal_predictors()) %&gt;%\n  step_normalize(all_numeric_predictors())\n\nIf you supply recipe = credit_rec to fastml() internal preprocessing (imputation/encoding/scaling) is skipped, preventing conflicts.\n\n\n## 3 Â· Tuning Strategies\n### 3.1 Bayesian Optimisation\nbayes_res &lt;- fastml(\ndata       = credit_data,\nlabel      = \"Status\",\nalgorithms = c(\"rand_forest\", \"xgboost\"),\nmetric     = \"roc_auc\",\nresampling_method = \"cv\",\nfolds      = 5,\nrecipe     = credit_rec,\ntuning_strategy   = \"bayes\",\ntuning_iterations = 40,      # iterations after the 10â€‘point spaceâ€‘filling start\nearly_stopping    = TRUE,    # stop when no improvement for 5 iters\nn_cores           = 6,\nseed              = 2025)\nBayesian search explores the hyperâ€‘parameter space efficiently, especially when evaluation is costly.\n\n3.2 Adaptive Racing (ANOVA)\nrace_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  recipe     = credit_rec,\n  metric     = \"roc_auc\",\n  adaptive   = TRUE,           # enables `finetune::tune_race_anova()`\n  tuning_strategy = \"grid\",   # initial grid; racing drops losers\n  folds      = 10,\n  n_cores    = 6,\n  seed       = 2025)\nRacing aggressively prunes poor combos early; handy for large grids.\n\n## 4 Â· Parallel & Distributed Engines\n### 4.1 Multiâ€‘core (doFuture)\nfastml() automatically parallelises when n_cores &gt; 1:\nlibrary(doFuture)\nregisterDoFuture()\n\nplan(multisession, workers = 8)  # or multicore on Linux/macOS\n\n\n4.2 Spark Cluster\nlibrary(sparklyr)\nsc &lt;- spark_connect(master = \"spark://myâ€‘cluster:7077\")\n\nspark_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"linear_reg\"),\n  algorithm_engines = list(rand_forest = \"spark\",\n                           linear_reg  = \"spark\"),\n  metric     = \"roc_auc\",\n  n_cores    = 4)  # number of executor cores per worker\n\nSpark engines (sparklyr wrappers) offload model fitting to the cluster while fastml coordinates.\n\n\n\n4.3 H2O AutoML + fastml API\nlibrary(h2o)\nh2o.init(nthreads = -1)\n\nh2o_res &lt;- fastml(\n  data       = credit_data,\n  label      = \"Status\",\n  algorithms = c(\"rand_forest\", \"logistic_reg\"),\n  algorithm_engines = list(rand_forest  = \"h2o\",\n                           logistic_reg = \"h2o\"),\n  metric     = \"roc_auc\",\n  impute_method = \"h2o\",   # optional custom imputation function\n  n_cores    = 4)\n\n## 5 Â· Learning Curves & Monitoring\nlc_res &lt;- fastml(\ndata           = credit_data,\nlabel          = \"Status\",\nalgorithms     = c(\"xgboost\"),\nlearning_curve = TRUE,\nfolds          = 5)\nThe builtâ€‘in learning curve shows ROC AUC vs training fraction, helping spot underâ€‘/overâ€‘fitting.\n\n## 6 Â· Ensembling / Stacking\nWhile fastml() returns individual models, you can ensemble their predictions easily.\npred_dfs &lt;- bayes_res$predictions  # nested list\n\n# Extract probabilities for the positive class across models\nprobs &lt;- purrr::map_df(pred_dfs, ~ .x[[1]][, c(\".pred_bad\")], .id = \"Model\") %&gt;%\n  bind_cols(truth = rep(credit_data$Status, times = length(pred_dfs)))\n\n# Simple average ensemble\nprobs$avg &lt;- probs %&gt;%\n  select(starts_with(\".pred_\")) %&gt;%\n  rowMeans()\n\nlibrary(yardstick)\nroc_auc(probs, truth = truth, avg)\nFor a tidy stacking pipeline use the stacks package and supply fastml workflows as candidates.\n\n## 7 Â· Custom Metrics\nPass any yardstick-compatible summariser via summaryFunction.\ngeometric_mean &lt;- function(data, ...) {\n  yardstick::mcc(data, ...)  # Matthewâ€™s correlation coefficient as example\n}\n\ncustom_res &lt;- fastml(\n  data      = credit_data,\n  label     = \"Status\",\n  algorithms = c(\"rand_forest\", \"xgboost\"),\n  summaryFunction = geometric_mean,\n  metric    = \"mcc\")\n\n## 8 Â· Save & Reload Best Model\nbest_wf &lt;- bayes_res$best_model[[1]]\n\ntidymodels::write_rds(best_wf, \"models/best_credit_model.rds\")\n\n# later...\nloaded_wf &lt;- readr::read_rds(\"models/best_credit_model.rds\")\npredict(loaded_wf, new_data = credit_data[1:3, ])\n\n## 9 Â· Frequently Asked Questions\n\n\n\n\n\n\n\nQuestion\nAnswer\n\n\n\n\nâ€œCan I tune across multiple engines for the same algorithm?â€\nYes â€“ supply algorithm_engines = list(rand_forest = c(\"ranger\", \"partykit\")). Each engine is tuned independently.\n\n\nâ€œWhy does tuning seem slow?â€\nCheck n_cores, reduce folds, or switch to adaptive racing.\n\n\nâ€œHow do I use a grouped timeâ€‘series CV?â€\nPrepare rsample::vfold_cv object yourself and pass via resampling_method = \"none\", then supply resamples (coming in next release).\n\n\n\n\n## 10 Â· Where to Next?\n\nExplainability: revisit fastexplain() with SHAP & PDPs on your best model.\nProduction: convert workflow to a vetiver model for API deployment.\nAutomated monitoring: schedule fastml reâ€‘training with GitHub Actions or cronR.\n\nHappy modelling!\n\n### Session Info\nsessionInfo()"
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutorials",
    "section": "",
    "text": "Welcome to the tutorials page! Choose a tutorial below to start learning how to use the fastml package for machine learning tasks, ranging from data exploration to advanced techniques.\n\n\nğŸ” Exploring Data\nGet a feel for your dataset, visualize missing values, spot outliers, and understand correlations in one call.\nStart the fastexplore() guide Â»\n\n\nğŸ·ï¸ Classification Workflows\nTune and compare dozens of algorithms for categorical outcomes.\nJump to fastml-classification Â»\n\n\nğŸ“ˆ Regression Workflows\nAutomate numeric-target modeling with cross-validation, grid searches, and performance plots.\nJump to fastml-regression Â»\n\n\nğŸš€ Advanced Techniques\nLearn about Bayesian & racing searches, parallel processing engines (H2O/Spark), ensembling, and learning curves.\nExplore fastml-advanced Â»\n\n\nğŸ” Model Explainability\nUnderstand model predictions with permutation variable importance, SHAP values, and profile plots.\nOpen the fastexplain() guide Â»\n\n\n\nNeed a refresher on installing fastml?\nSee the Getting Started page for installation, basic usage, and package philosophy."
  },
  {
    "objectID": "tutorials/fastml-classification.html",
    "href": "tutorials/fastml-classification.html",
    "title": "Training Classification Models with fastml()",
    "section": "",
    "text": "# Overview\nThe fastml() function is the core of the fastml package. It automates the process of:\n\ndata splitting\npreprocessing (recipes)\ncrossâ€‘validation / resampling\nhyperâ€‘parameter tuning\nmodel fitting for many algorithms and engines\nperformance comparison\n\nIn this tutorial weâ€™ll walk through a classification workflow from start to finish. A followâ€‘up tutorial covers regression.\n\n## 1Â Â·Â Load Packages and Data\nlibrary(fastml)\nlibrary(dplyr)\n\n# Binary classification version of iris\niris_bin &lt;- iris %&gt;%\nfilter(Species != \"setosa\") %&gt;%\nmutate(Species = factor(Species))\nWhy binary? fastml() supports multiâ€‘class too, but binary keeps plots concise for a first demo.\n\n## 2Â Â·Â Inspect With fastexplore() (optional)\nfastexplore(iris_bin, label = \"Species\", plot = TRUE)\nUse the output to spot class imbalance, missing values, or highlyâ€‘correlated predictors before training.\n\n## 3Â Â·Â Train Many Models in One Shot\nset.seed(123)\nresult &lt;- fastml(\ndata        = iris_bin,\nlabel       = \"Species\",\nalgorithms  = c(\"logistic_reg\",           # classical baseline\n                \"svm_rbf\",                # nonâ€‘linear margin\n                \"rand_forest\",            # ensemble tree\n                \"xgboost\"),               # gradient boosting\nmetric      = \"accuracy\",                # optimise this\nresampling_method = \"cv\",                # 10â€‘fold CV by default\nfolds       = 10,\ntune        = TRUE,                        # hyperâ€‘parameter search\nn_cores     = 4                            # parallel speedâ€‘up\n)\n\nWhat just happened?\n\n\n\n\n\n\n\nStep\nAutomated task\n\n\n\n\n1\nStratified train/test split (80â€¯/â€¯20)\n\n\n2\nRecipe creation: dummyâ€‘encoding, centring, scaling, zeroâ€‘variance removal\n\n\n3\n10â€‘fold crossâ€‘validation inside the training fold\n\n\n4\nModelâ€‘specific tuning grids (or Bayesian search if tuning_strategy = \"bayes\")\n\n\n5\nMetrics collected and the best workflow finalised\n\n\n\n\n## 4Â Â·Â Compare Models\nsummary(result, plot = TRUE)\n\nTable of metrics (Accuracy, Kappa, F1,Â â€¦)\nBarâ€‘plot faceted by metric\nROC curves (binary) or macroâ€‘ROC (multiclass)\nConfusion matrix for the best model\n\n\nTip Â Use algorithm = c(\"rand_forest\", \"svm_rbf\") to zoom in on a subset.\n\n\n## 5Â Â·Â Inspect the Best Model\nresult$best_model_name          # Which engine won?\nprint(result$best_model[[1]])   # Workflow details\n\n\nPredictions on New Data\nnew_obs &lt;- iris_bin %&gt;% slice_sample(n = 5)\n\npredict(result$best_model[[1]],  new_obs, type = \"class\")\npredict(result$best_model[[1]],  new_obs, type = \"prob\")\n\n## 6Â Â·Â Model Explainability\nCombine with fastexplain() to interpret:\nfastexplain(result,\n          type = \"importance\",     # or \"shap\", \"profiles\", \"full\"\n          vi_iterations = 20,       # more stable importance\n          shap_sample   = 10)       # larger SHAP sample\nOutputs include permutationâ€‘based variable importance, SHAP summary, and partialâ€‘dependence profiles.\n\n## 7Â Â·Â Advanced Options Cheatâ€‘Sheet\n\n\n\n\n\n\n\n\nArgument\nPurpose\nExample\n\n\n\n\nrecipe\nSupply a recipes object; overrides autoâ€‘recipe\nrecipe(Species ~ ., data = iris_bin) %&gt;% step_normalize(all_numeric())\n\n\nimpute_method\nHandle NAsÂ â€” â€œmedianImputeâ€, â€œmiceâ€, â€œmissForestâ€, â€¦\nimpute_method = \"knnImpute\"\n\n\nalgorithm_engines\nChange engines per algorithm\nlist(rand_forest = \"randomForest\", svm_rbf = \"kernlab\")\n\n\ntuning_strategy\nâ€œgridâ€, â€œbayesâ€ (with early_stopping)\ntuning_strategy = \"bayes\", tuning_iterations = 25\n\n\nlearning_curve\nPlot performance vsÂ training size\nlearning_curve = TRUE\n\n\n\nSee ?fastml for the full argument list.\n\n## 8Â Â·Â NextÂ Steps\n\nRegression Tutorial â€“ predicting continuous outcomes.\nAdvanced Customisation â€“ stacking, ensembling, and adaptive racing.\nPerformance at Scale â€“ parallel & distributed engines (spark, h2o).\n\nJump to the next tutorial: Regression with fastml()\n\n### SessionÂ Info\nsessionInfo()"
  },
  {
    "objectID": "getting-started.html",
    "href": "getting-started.html",
    "title": "Getting Started with fastml",
    "section": "",
    "text": "Introduction\nThe fastml package provides a streamlined, user-friendly interface for training, evaluating, and comparing multiple machine learning models for classification or regression tasks. Built on top of the tidymodels ecosystem, it automates common steps like resampling, preprocessing, model tuning, and performance reporting, so you can focus on insights rather than infrastructure.\nThis tutorial will help you get started with fastml, explaining how to install the package, prepare your data, train multiple models in one command, and inspect their performance.\n\n\nInstallation\nInstall the latest stable version from CRAN:\ninstall.packages(\"fastml\")\nYou can install the development version from GitHub:\n# install.packages(\"devtools\")\ndevtools::install_github(\"selcukkorkmaz/fastml\")\nOnce installed, load the package:\nlibrary(fastml)\n\n\nExample Dataset\nTo illustrate how fastml works, weâ€™ll use the built-in iris dataset. Weâ€™ll convert it into a binary classification task for simplicity:\nlibrary(dplyr)\ndata &lt;- iris %&gt;%\n  filter(Species != \"setosa\") %&gt;%\n  mutate(Species = factor(Species))\n\n\nRunning fastml()\nThe main function in this package is fastml(). It takes in a dataset and automates model training for a selected list of algorithms.\nresult &lt;- fastml(\n  data = data,\n  label = \"Species\",\n  task = \"classification\",\n  metric = \"accuracy\",\n  algorithms = c(\"logistic_reg\", \"rand_forest\", \"svm_linear\"),\n  nfolds = 5,\n  tune = TRUE,\n  seed = 123\n)\n\nKey Arguments Explained:\n\ndata: A data frame containing predictors and the outcome.\nlabel: The name of the outcome column (character).\ntask: Either â€œclassificationâ€ or â€œregressionâ€.\nmetric: The performance metric to optimize (e.g., â€œaccuracyâ€, â€œrmseâ€).\nalgorithms: A vector of algorithm names. Use availableMethods(\"classification\") to see all.\nnfolds: Number of folds for cross-validation.\ntune: Whether to perform hyperparameter tuning (TRUE/FALSE).\nseed: Reproducibility seed.\n\n\n\n\nOutput\nThe result is a list with the following key elements:\n\nmodels: Nested list of fitted model workflows.\nperformance: Performance metrics for each model-engine pair.\npredictions: Predictions and class probabilities.\nbest_model_name: Best engine for each algorithm.\nbest_model: Finalized best model workflows.\n\n\n\nSummary of Results\nYou can generate a comprehensive summary with:\nsummary(result)\nThis command prints model-wise performance metrics, highlights the best model, and produces comparison plots.\n\n\nNext Steps\nOnce youâ€™ve trained models using fastml(), you can:\n\nUse summary() with plot = TRUE to visualize ROC curves, confusion matrices, and more.\nRun fastexplain() to compute variable importance and SHAP values (explained in another tutorial).\n\nIn the next tutorial, we will explain in detail how fastml handles model training, including resampling, engine selection, and hyperparameter tuning."
  },
  {
    "objectID": "tutorials/fastexplore.html",
    "href": "tutorials/fastexplore.html",
    "title": "Exploring Data with fastexplore()",
    "section": "",
    "text": "# Introduction\nBefore building predictive models, itâ€™s essential to understand your dataset. The fastexplore() function in the fastml package provides a quick and informative overview of your data. It computes descriptive statistics, detects missing values, highlights class imbalance, and gives early insights into feature distributions and correlations.\nIn this tutorial, weâ€™ll explore how to use fastexplore() to inspect datasets before model training.\n\nLoading the Package and Data\nWeâ€™ll use the classic PimaIndiansDiabetes dataset from the mlbench package to demonstrate.\nlibrary(fastml)\nlibrary(mlbench)\ndata(PimaIndiansDiabetes)\n\n# View structure\nstr(PimaIndiansDiabetes)\n\n\nUsing fastexplore()\nThe fastexplore() function expects a data frame and the name of the outcome column. It works for both classification and regression problems.\nexplore &lt;- fastexplore(\n  data = PimaIndiansDiabetes,\n  label = \"diabetes\"\n)\n\n\nOutput Summary\nThe result is a list with structured summaries:\n\nbasic: General info like number of rows, columns, and missing values.\ntarget: Outcome distribution.\nfeatures: Summary statistics for each feature.\ncorrelation: Feature correlation matrix (for numeric predictors).\n\nYou can inspect these components individually:\nexplore$basic\nexplore$target\nexplore$features\nexplore$correlation\n\n\nVisualizing Exploration Results\nfastexplore() also provides an optional argument plot = TRUE to visualize results.\nfastexplore(\n  data = PimaIndiansDiabetes,\n  label = \"diabetes\",\n  plot = TRUE\n)\nThis generates:\n\nA bar plot of outcome class distribution.\nHistograms of numerical features.\nA heatmap of pairwise correlations.\n\n\n\nWhen to Use fastexplore()\nUse fastexplore():\n\nRight after loading your dataset.\nBefore splitting the data.\nTo detect class imbalance or outliers.\nTo guide feature engineering decisions.\n\n\n\nConclusion\nThe fastexplore() function helps you quickly assess your datasetâ€™s structure and quality. Understanding your data is the first step toward building a robust machine learning model. In the next tutorial, weâ€™ll demonstrate how to train and compare models using fastml()."
  },
  {
    "objectID": "tutorials/fastexplain.html",
    "href": "tutorials/fastexplain.html",
    "title": "Explaining Models with fastexplain()",
    "section": "",
    "text": "Introduction\nAfter training your models using the fastml() function, you can gain deeper insights into model behavior using the fastexplain() function. This function provides a set of visual and quantitative tools to help you interpret model predictions, understand feature importance, and explore the relationships between predictors and the outcome.\nThe fastexplain() function is built on top of the DALEX package, allowing for model-agnostic explanations that work across a wide range of algorithms.\n\n\nPurpose\nfastexplain() helps to answer questions such as:\n\nWhich features are the most important for model prediction?\nHow do features affect the predicted outcome?\nAre model predictions stable or sensitive to small changes in input?\n\n# Usage\nexplanation &lt;- fastexplain(result, type = \"full\")\n\nKey Arguments:\n\nobject: The output object from fastml().\ntype: Type of explanation. Options are:\n\n\"importance\": Show variable importance only\n\n\"shap\": Show SHAP values only\n\"profiles\": Show model profile plots\n\"full\": Display all available explanation types\nmodel_index: Optional index if you want to explain a model other than the best one\n\n\n\n\nExample\nWeâ€™ll continue using the result object from the fastml() example in the previous tutorial:\nexplanation &lt;- fastexplain(result, type = \"full\")\nThis call produces:\n\nA variable importance plot (permutation-based)\nA SHAP summary plot\nModel profile plots (how individual features influence predictions)\n\n\n\nVisual Outputs\nEach of the outputs helps you understand different aspects:\n\nVariable Importance: Shows which variables contribute most to the modelâ€™s predictive performance.\nSHAP Summary: Breaks down individual predictions by showing how much each feature contributed.\nModel Profiles: Depicts the functional form of how features affect predictions (analogous to partial dependence plots).\n\n\n\nCustomization\nYou can pass additional arguments to customize behavior. For example:\nfastexplain(result, type = \"importance\", top_n = 10)\nLimits the explanation to the top 10 most important features.\n\n\nNotes\nMake sure that your data contains enough features and variation to compute reliable explanations. For very small datasets, some explanation techniques may produce unstable results.\n\n\nNext Steps\nIn the next tutorial, weâ€™ll explore how to perform model stacking and ensembling in fastml to further improve predictive performance."
  },
  {
    "objectID": "tutorials/fastml-regression.html",
    "href": "tutorials/fastml-regression.html",
    "title": "Predicting Continuous Outcomes with fastml()",
    "section": "",
    "text": "# Overview\nIn this tutorial weâ€™ll use fastml to tackle a regression problem: predicting house sale prices. Youâ€™ll see how fastml() automates preprocessing, resampling, tuning, and model comparison for continuous targets just as effortlessly as for classification.\nNext stop after this tutorial: advanced topics (stacking, ensembling, adaptive racing).\n\n## 1Â Â· Load Packages and Data\nlibrary(fastml)\nlibrary(dplyr)\nlibrary(modeldata)   # provides the Ames housing data\n\ndata(ames)\names(ames)[names(ames) == \"Sale_Price\"] &lt;- \"Sale_Price\"  # target already named\n\nset.seed(123)  # reproducibility\nThe Ames dataset (ames) contains ~2â€¯900 rows and 80 predictors describing Iowa homes.\n\n## 2Â Â· Quick EDA with fastexplore() (optional but useful)\nfastexplore(ames, label = \"Sale_Price\",\n          visualize = c(\"histogram\", \"boxplot\", \"heatmap\"),\n          save_results = FALSE,\n          sample_size  = 1000)   # sample to keep plotting fast\nExplore distributions, missingness, and correlations before modelling.\n\n## 3Â Â· Train Several Regression Models\nWeâ€™ll compare a mix of linear and nonâ€‘linear algorithms.\nreg_result &lt;- fastml(\n  data        = ames,\n  label       = \"Sale_Price\",\n  algorithms  = c(\"linear_reg\",      # baseline OLSE\n                  \"elastic_net\",    # regularised linear\n                  \"rand_forest\",    # tree ensemble\n                  \"xgboost\",        # gradient boosting\n                  \"svm_rbf\",        # kernel SVM\n                  \"pls\"),           # partial least squares\n  metric      = \"rmse\",            # lower is better\n  resampling_method = \"cv\",\n  folds       = 10,\n  tune        = TRUE,               # grid tuning by default\n  n_cores     = 6,\n  impute_method = \"medianImpute\",  # Ames has some NA values\n  seed        = 123)\n\nUnder the Hood\n\nRecipe: medianâ€‘imputes NAs, oneâ€‘hotâ€‘encodes categoricals, centres & scales numerics.\nResampling: 10â€‘fold CV within the train split.\nTuning grids: automatically generated per algorithm.\nFinalisation: the best hyperâ€‘parameters are selected and the workflow refit on the full training data.\n\n\n## 4Â Â· Compare Model Performance\nsummary(reg_result, plot = TRUE)\n\nTable of RMSE / Râ€‘squared / MAE for each algorithmâ€‘engine pair.\nBarâ€‘plots faceted by metric.\nResidual diagnostics (truth vs predicted, histogram of residuals) for the best model.\n\n\nTip: Lower RMSE indicates better fit; higher Râ€‘squared indicates more variance explained.\n\n\n## 5Â Â· Inspect the Best Model\nreg_result$best_model_name      # which engine won?\n\nbest_wf &lt;- reg_result$best_model[[1]]\nbest_wf\n\n\nPredict on New Observations\nnew_homes &lt;- ames %&gt;% slice_sample(n = 5)\n\npredict(best_wf, new_homes)\nThe predictions are numeric saleâ€‘price estimates.\n\n## 6Â Â· Variable Importance & SHAP Values\nfastexplain(reg_result,\n          vi_iterations = 30,\n          shap_sample   = 50)\n\nPermutationâ€‘based variable importance with uncertainty bands.\nSHAP summary bar plot: average absolute impact of each feature.\nPartialâ€‘dependence profiles for top drivers.\n\n\n## 7Â Â· Learning Curves (optional)\nHow much data do we need before returns diminish?\nreg_result_lc &lt;- fastml(\ndata            = ames,\nlabel           = \"Sale_Price\",\nalgorithms      = c(\"rand_forest\", \"xgboost\"),\nmetric          = \"rmse\",\nlearning_curve  = TRUE,\nfolds           = 5,\nn_cores         = 6,\ntune            = TRUE,\nseed            = 123)\nLook for the elbow in the RMSE vs trainingâ€‘fraction plot to judge data sufficiency.\n\n## 8Â Â· Advanced Options Cheatâ€‘Sheet (Regression)\n\n\n\n\n\n\n\n\nArgument\nPurpose\nExample\n\n\n\n\nscaling_methods\nChoose centre / scale steps\nscaling_methods = NULL to skip\n\n\ntuning_strategy\nBayesian search\ntuning_strategy = \"bayes\", tuning_iterations = 40\n\n\nadaptive = TRUE\nRacing / ANOVA pruning\nadaptive = TRUE\n\n\nalgorithm_engines\nChange engines\nlist(rand_forest = \"ranger\", xgboost = \"xgboost\")\n\n\n\n\n## 9Â Â· Wrapâ€‘up & Next Steps\n\nExperiment with more algorithms via availableMethods(\"regression\").\nTry alternative metrics: metric = \"mae\" or a custom summary function.\nScale out using distributed engines (spark, h2o).\n\nMove on to the Advanced fastml Tutorial to learn about ensembling and custom recipes.\n\n### Session Info\nsessionInfo()"
  }
]